{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZXrexlP3spL"
      },
      "source": [
        "Lab 42 Implement Fine-Tuning Pretrained Models (GPT-2)â€‹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpUYtlF63spR"
      },
      "source": [
        "#### 1. Install Dependencies\n",
        "First, ensure you have the necessary libraries installed\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srmwnr8Y3spR"
      },
      "outputs": [],
      "source": [
        "pip install torch transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "KGiB7xMp3spR"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "t1TP9R693spS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load the CSV file into a pandas dataframe\n",
        "df = pd.read_csv('covid_data.csv')\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CovidDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "        input_text = f\"Q: {question} A: {answer}\"\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_attention_mask=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMRdsY7O3spS"
      },
      "source": [
        "#### 2. Load Pretrained  Model and Tokenizer\n",
        "Import the required libraries and load the pretrained BERT model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK866tH93spS",
        "outputId": "38811450-3c1f-4261-dea0-35f962b6fdfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Constants\n",
        "MODEL_NAME = \"gpt2\"\n",
        "LEARNING_RATE = 5e-5\n",
        "EPOCHS = 10\n",
        "WARMUP_STEPS = 5\n",
        "OUTPUT_DIR = \"/content/fine_tuned_gpt2\"\n",
        "BATCH_SIZE = 4\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "# Load pretrained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = CovidDataset(df, tokenizer, MAX_LENGTH)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Move model to the device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "num_training_steps = EPOCHS * len(dataloader)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=num_training_steps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU3tX6Zg3spU"
      },
      "source": [
        "### Model training\n",
        "\n",
        "I will train the model and save the model weights after each epoch and then I will try to generate jokes with each version of the weight to see which performs the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqVGQjRm3spU",
        "outputId": "67fb6eeb-0b32-4c43-8968-809710ac81a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Epoch loss: 7.082181167602539\n",
            "Epoch 2/10\n",
            "Epoch loss: 1.651819372177124\n",
            "Epoch 3/10\n",
            "Epoch loss: 1.0214077711105347\n",
            "Epoch 4/10\n",
            "Epoch loss: 0.8959383606910706\n",
            "Epoch 5/10\n",
            "Epoch loss: 0.7755052328109742\n",
            "Epoch 6/10\n",
            "Epoch loss: 0.6961182832717896\n",
            "Epoch 7/10\n",
            "Epoch loss: 0.636821174621582\n",
            "Epoch 8/10\n",
            "Epoch loss: 0.6069189548492432\n",
            "Epoch 9/10\n",
            "Epoch loss: 0.5808164954185486\n",
            "Epoch 10/10\n",
            "Epoch loss: 0.5646387696266174\n",
            "Model saved to /content/fine_tuned_gpt2\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    epoch_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch loss: {epoch_loss / len(dataloader)}\")\n",
        "\n",
        "# Save the model\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCvt30R-3spV"
      },
      "source": [
        "### Generating responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5KbKDeW3spV",
        "outputId": "91124ba9-b50d-4a00-c87c-c7df9f959ea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response: Q: What is Covid 19? A: Covids are a virus that can spread through the body through contact with the virus.\n"
          ]
        }
      ],
      "source": [
        "def generate_text(prompt, model, tokenizer, max_length=150, temperature=0.7, top_k=50, top_p=0.9):\n",
        "    input_text = f\"Q: {prompt} A:\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
        "            early_stopping=True\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"What is Covid 19?\"\n",
        "\n",
        "# Generate a response\n",
        "response = generate_text(prompt, model, tokenizer)\n",
        "print(f\"Generated response: {response}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}